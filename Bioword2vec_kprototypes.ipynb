{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cImkluvhDcOS"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrHe7IOYqR4W"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NJchMiOjJGd"
      },
      "outputs": [],
      "source": [
        "!pip install kmodes\n",
        "from kmodes.kprototypes import KPrototypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie3G8S6hDy2B"
      },
      "source": [
        "## Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj26FeESD2Zw"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQc-CMfaFsmW"
      },
      "source": [
        "## Preprocess function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NkMhnlyF9yn"
      },
      "outputs": [],
      "source": [
        "def load_stop_words(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        stop_words = set(line.strip() for line in file)\n",
        "    return stop_words\n",
        "\n",
        "stop_words_file = 'clinical-stopwords.txt'\n",
        "custom_stop_words = load_stop_words(stop_words_file)\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if t not in custom_stop_words and t not in punctuation]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Qccn5dJtW7"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH7_32rFJv58"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('synthetic_data_2021.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF_rsNyBKYPJ"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV7n2XjHbu-8"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = pd.to_datetime(dataset['date'], format='%m/%d/%y', errors='coerce')\n",
        "print(dataset['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ET0eYLSbzF9"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = pd.to_datetime(dataset['date'], format='%Y/%m/%d', errors='coerce')\n",
        "weekly_groups = dataset.groupby(pd.Grouper(key='date', freq='W-FRI'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0TtTqYAb5QY"
      },
      "outputs": [],
      "source": [
        "for week, data in weekly_groups:\n",
        "    print(f\"Week ending on {week}\")\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdkv-jpae38l"
      },
      "outputs": [],
      "source": [
        "X = dataset.iloc[:, [0,2,3,4]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ27G-HJKb2S"
      },
      "outputs": [],
      "source": [
        "processed_data = [preprocess(complaint) for complaint in X]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-f6ZC98p7as"
      },
      "outputs": [],
      "source": [
        "!wget -O BioWordVec_PubMed_MIMICIII_d200.vec.bin 'https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/BioSentVec/BioWordVec_PubMed_MIMICIII_d200.vec.bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jOP3NszlTX5"
      },
      "outputs": [],
      "source": [
        "biowordvec_path = 'BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zad9BzrIL-Tf"
      },
      "outputs": [],
      "source": [
        "def get_vector(text, model):\n",
        "    words = preprocess(text)\n",
        "    word_vectors = [model[word] for word in words if word in model]\n",
        "    if word_vectors:\n",
        "        vector = sum(word_vectors) / len(word_vectors)\n",
        "        return vector\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NimcluQ7MGfZ"
      },
      "outputs": [],
      "source": [
        "vectors = []\n",
        "valid_complaints = []\n",
        "for complaint in X.iloc[:,3]:\n",
        "    vector = get_vector(complaint, model)\n",
        "    if vector is not None:\n",
        "        vectors.append(vector)\n",
        "        valid_complaints.append(complaint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HxQWSjSsPOji"
      },
      "outputs": [],
      "source": [
        "print(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxSU15rTobRY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PisYjGUwo-lc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Scree Plot')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OnDyv2XQpRKD"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=130)  # e.g., 50\n",
        "reduced_vectors = pca.fit_transform(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PA_5JsYiwpl_"
      },
      "outputs": [],
      "source": [
        "X = X.join(pd.DataFrame(reduced_vectors))\n",
        "#X.to_csv('vectorized.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOgkle0C6sI2"
      },
      "outputs": [],
      "source": [
        "def column_index(df, query_cols):\n",
        "  '''\n",
        "  returns the indices of the query_cols\n",
        "  '''\n",
        "  cols = df.columns.astype(str)\n",
        "  sidx = np.argsort(cols)\n",
        "  return sidx[np.searchsorted(cols,query_cols.astype(str),sorter=sidx)]\n",
        "\n",
        "\n",
        "cat_cols = X.iloc[:, [1,2]]\n",
        "categorical_indices = column_index(X, cat_cols.columns)\n",
        "categorical = list(categorical_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttPZvxu0lCw0"
      },
      "outputs": [],
      "source": [
        "print(cat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohYGLdIBtlQK"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEglFtoNhU9B"
      },
      "outputs": [],
      "source": [
        "weekly_groups = X.groupby(pd.Grouper(key='date', freq='W-WED'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zkeqRaqksGF"
      },
      "outputs": [],
      "source": [
        "data_for_clustering['hospcode'] = data_for_clustering['hospcode'].astype(str)\n",
        "data_for_clustering['agegroup'] = data_for_clustering['agegroup'].astype(str)\n",
        "categorical_columns = data_for_clustering.select_dtypes(include='object').columns\n",
        "categorical_indices = [data_for_clustering.columns.get_loc(col) for col in categorical_columns]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey9simHERdHz"
      },
      "outputs": [],
      "source": [
        "data_for_clustering = weekly_data.drop(['date', 'cc'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWC2EWp2Lx5G"
      },
      "source": [
        "## Using the elbow method to find the optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X1tGszocNo67"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "wcss = []\n",
        "for i in range(5, 30):\n",
        "    kproto = KPrototypes(n_clusters=i, init='Cao',\n",
        "                     n_jobs = 1, verbose=0,\n",
        "                     random_state=42)\n",
        "    kproto.fit_predict(X, categorical=categorical)\n",
        "    wcss.append(kproto.cost_)\n",
        "plt.plot(range(5, 30), wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgT0mANLL4Nz"
      },
      "source": [
        "## Training the K-Means model on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjEfU6ZSMAPl"
      },
      "outputs": [],
      "source": [
        "\n",
        "clustered_data = []\n",
        "\n",
        "# Step 4: Loop through each group of weekly data\n",
        "for week_start, weekly_data in weekly_groups:\n",
        "    kproto = KPrototypes(n_clusters=12, init='Cao',\n",
        "                     n_jobs = 4, verbose=0,\n",
        "                     random_state=42)\n",
        "    data_for_clustering = weekly_data.drop(['date', 'cc'], axis=1)\n",
        "    data_for_clustering['hospcode'] = data_for_clustering['hospcode'].astype(str)\n",
        "    data_for_clustering['agegroup'] = data_for_clustering['agegroup'].astype(str)\n",
        "    data_for_clustering = data_for_clustering.dropna()\n",
        "    categorical_columns = data_for_clustering.select_dtypes(include='object').columns\n",
        "    categorical_indices = [data_for_clustering.columns.get_loc(col) for col in categorical_columns]\n",
        "    # Fit and predict clusters\n",
        "    clusters = kproto.fit_predict(data_for_clustering, categorical=categorical_indices)\n",
        "\n",
        "    weekly_data_cleaned = weekly_data.loc[data_for_clustering.index]  # Align with rows used in clustering\n",
        "    weekly_data_cleaned.loc[:, 'cluster'] = clusters  # Add the clusters only to cleaned data\n",
        "\n",
        "    # Append the cleaned weekly data with clusters to the list\n",
        "    clustered_data.append(weekly_data_cleaned)\n",
        "\n",
        "# Concatenate all weekly clustered data into a single DataFrame\n",
        "clustered_dataset = pd.concat(clustered_data)\n",
        "\n",
        "# Step 6: Display the result\n",
        "print(clustered_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6xqXqOSOGOY"
      },
      "outputs": [],
      "source": [
        "len(clustered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGwxyetlEOm7"
      },
      "outputs": [],
      "source": [
        "X = pd.merge(X, pd.DataFrame(clusters), left_index=True, right_index=True) # Merge based on indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad9z09dua5JM"
      },
      "outputs": [],
      "source": [
        "X.rename(columns={'0_y': 'cluster', '0_x': '0'}, inplace=True)\n",
        "X.to_csv('clustered.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r9ju_deQDNx"
      },
      "source": [
        "## Visualising the clusters in 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlKl5K2KP1C9"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Assuming X is a pandas DataFrame, convert it to a NumPy array\n",
        "#X_dense = X.values\n",
        "\n",
        "# Reduce dimensions (here using PCA for demonstration; consider t-SNE or MDS for better handling of categorical variables)\n",
        "pca = PCA(n_components=2)\n",
        "vectors_pca = pca.fit_transform(vectors) # Pass the dense array here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSQBmbbiUYZ-"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of columns for subplots (adjust based on how many weekly plots you want per row)\n",
        "n_cols = 3  # Number of subplots per row\n",
        "n_weeks = len(clustered_data)  # Total number of weekly groups\n",
        "\n",
        "# Calculate the number of rows needed\n",
        "n_rows = int(np.ceil(n_weeks / n_cols))\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  # Adjust figure size based on rows and cols\n",
        "axs = axs.flatten()  # Flatten axes for easier indexing\n",
        "\n",
        "# Initialize LabelEncoder for categorical data\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Loop over each weekly dataset and visualize clusters\n",
        "for i, weekly_data in enumerate(clustered_data):\n",
        "    # Extract features and clusters\n",
        "    data_for_clustering = weekly_data.drop(columns=['cluster', 'date', 'cc'])\n",
        "\n",
        "    # Apply Label Encoding for categorical columns (no one-hot encoding)\n",
        "    categorical_columns = ['hospcode', 'agegroup']  # Replace with actual categorical columns\n",
        "    for col in categorical_columns:\n",
        "        data_for_clustering[col] = encoder.fit_transform(data_for_clustering[col])\n",
        "\n",
        "    # Extract numeric values (including encoded categorical data)\n",
        "    vectors = data_for_clustering.values  # Use the full dataset with label-encoded categorical variables\n",
        "    clusters = weekly_data['cluster'].values  # Cluster labels\n",
        "\n",
        "    # Apply PCA (or t-SNE) to reduce dimensions to 2\n",
        "    pca = PCA(n_components=2)\n",
        "    vectors_pca = pca.fit_transform(vectors)  # Reduce dimensions to 2\n",
        "\n",
        "    # Scatter plot of clusters\n",
        "    axs[i].scatter(vectors_pca[:, 0], vectors_pca[:, 1], c=clusters, cmap='viridis', label='Cluster ID')\n",
        "    axs[i].set_title(f'Week {i + 1}')\n",
        "    axs[i].set_xlabel('PC1')\n",
        "    axs[i].set_ylabel('PC2')\n",
        "\n",
        "# Hide any empty subplots if they exist\n",
        "for j in range(i + 1, len(axs)):\n",
        "    fig.delaxes(axs[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6G8pmyUwSn"
      },
      "source": [
        "## Visualising the clusters in 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmo7x2vyUzoF"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to reduce dimensions to three\n",
        "pca = PCA(n_components=3)\n",
        "vectors_pca = pca.fit_transform(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEsJtZ8OAHZi"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of columns for subplots (adjust based on how many weekly plots you want per row)\n",
        "n_cols = 3  # Number of subplots per row\n",
        "n_weeks = len(clustered_data)  # Total number of weekly groups\n",
        "\n",
        "# Calculate the number of rows needed\n",
        "n_rows = int(np.ceil(n_weeks / n_cols))\n",
        "\n",
        "# Create subplots with 3D projection\n",
        "fig = plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "# Initialize LabelEncoder for categorical data\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Loop over each weekly dataset and visualize clusters\n",
        "for i, weekly_data in enumerate(clustered_data):\n",
        "    # Extract features and clusters\n",
        "    data_for_clustering = weekly_data.drop(columns=['cluster', 'date', 'cc'])\n",
        "\n",
        "    # Apply Label Encoding for categorical columns (no one-hot encoding)\n",
        "    categorical_columns = ['hospcode', 'agegroup']  # Replace with actual categorical columns\n",
        "    for col in categorical_columns:\n",
        "        data_for_clustering[col] = encoder.fit_transform(data_for_clustering[col])\n",
        "\n",
        "    # Extract numeric values (including encoded categorical data)\n",
        "    vectors = data_for_clustering.values  # Use the full dataset with label-encoded categorical variables\n",
        "    clusters = weekly_data['cluster'].values  # Cluster labels\n",
        "\n",
        "    # Apply PCA to reduce dimensions to 3\n",
        "    pca = PCA(n_components=3)\n",
        "    vectors_pca = pca.fit_transform(vectors)  # Reduce dimensions to 3\n",
        "\n",
        "    # Add a 3D subplot for each week\n",
        "    ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')\n",
        "\n",
        "    # Scatter plot in 3D\n",
        "    scatter = ax.scatter(vectors_pca[:, 0], vectors_pca[:, 1], vectors_pca[:, 2], c=clusters, cmap='viridis', label='Cluster ID')\n",
        "\n",
        "    # Set titles and labels\n",
        "    ax.set_title(f'Week {i + 1}')\n",
        "    ax.set_xlabel('Principal Component 1')\n",
        "    ax.set_ylabel('Principal Component 2')\n",
        "    ax.set_zlabel('Principal Component 3')\n",
        "\n",
        "# Add a color bar\n",
        "fig.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNIonTq0HZhPlB0IYiMbQ2u"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}