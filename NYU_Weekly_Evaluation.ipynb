{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECMA-oFQNepK"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x--kCjQP14T"
      },
      "outputs": [],
      "source": [
        "# Function to load and extract text from a .docx file\n",
        "def load_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    data = []\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            # Extract the cells' text\n",
        "            cells = [cell.text for cell in row.cells]\n",
        "            data.append(cells)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D45XH923QBOR"
      },
      "outputs": [],
      "source": [
        "# Function to identify if the complaint is related to tuberculosis\n",
        "def is_tuberculosis_related(complaint):\n",
        "    keywords = ['tuber']\n",
        "    return any(re.search(keyword, complaint.lower()) for keyword in keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwamv6lsQD68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Load the document\n",
        "data = pd.read_csv('/content/clustered_DBSCAN_eps=eps_50_4_wed.csv')  # Replace with your actual file path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "1z--AEcYcnEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qLJeTsr8e8q"
      },
      "outputs": [],
      "source": [
        "# Assuming the Chief Complaint is in the 4th column and Cluster Label in the 5th column\n",
        "#chief_complaints = [row[3] for row in data if len(row) > 1 and row[3]!= (\"cc\")]\n",
        "cluster_labels = data['cluster']\n",
        "chief_complaints = data['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLliHSjCSv2R"
      },
      "outputs": [],
      "source": [
        "len(cluster_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S21StLHUmI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8yPMZUyTPTY"
      },
      "outputs": [],
      "source": [
        "print(cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Iw5zLW744g"
      },
      "source": [
        "# Silhouette score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnnqDf-k8F4s"
      },
      "outputs": [],
      "source": [
        "#!pip install gower\n",
        "import pandas as pd\n",
        "import gower\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "# Assuming 'data_vectorized' has a 'date' column for timestamps and a 'cluster_labels' column\n",
        "# Drop columns that should not be part of clustering\n",
        "data_for_clustering = data\n",
        "data_for_clustering['week_start'] = pd.to_datetime(data_for_clustering['week_start'])\n",
        "weekly_groups = data_for_clustering.groupby('week_start')\n",
        "# Convert 'date' column to datetime if not already\n",
        "#data_for_clustering['date'] = pd.to_datetime(data_for_clustering['date'])\n",
        "\n",
        "# Group by week using pandas Grouper\n",
        "#weekly_groups = data_for_clustering.groupby(pd.Grouper(key='date', freq='W-MON'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gower\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gower\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "clustered_dataset = data\n",
        "\n",
        "def parse_vector_string(vector_str):\n",
        "    # Convert string representation of vector to numpy array\n",
        "    cleaned = vector_str.strip('[]').replace('\\n', ' ')\n",
        "    return np.array([float(x) for x in cleaned.split()])\n",
        "\n",
        "weekly_cc_per_cluster = {}\n",
        "\n",
        "# Loop through each week's data\n",
        "for week_start, week_data in clustered_dataset.groupby('week_start'):\n",
        "    print(f\"\\nProcessing week: {week_start}\")\n",
        "\n",
        "    try:\n",
        "        # Parse vectors from strings and stack them\n",
        "        vectors = np.vstack([parse_vector_string(str(v)) for v in week_data['reduced_vector'].values])\n",
        "\n",
        "        # Convert to DataFrame for gower distance\n",
        "        vectors_df = pd.DataFrame(\n",
        "            vectors,\n",
        "            columns=[f'dim_{i}' for i in range(vectors.shape[1])]\n",
        "        )\n",
        "\n",
        "        # Get cluster labels\n",
        "        week_cluster_labels = week_data['cluster'].values\n",
        "\n",
        "        # Compute silhouette scores, ignoring noise points (-1)\n",
        "        mask = week_cluster_labels != -1\n",
        "        non_noise_labels = set(week_cluster_labels[mask])\n",
        "\n",
        "        if len(non_noise_labels) >= 2 and mask.sum() > 0:\n",
        "            # Compute Gower distance matrix for non-noise points\n",
        "            print(f\"Computing Gower distances for {sum(mask)} non-noise points\")\n",
        "\n",
        "            # Select only non-noise points for distance calculation\n",
        "            vectors_df_filtered = vectors_df[mask].copy()\n",
        "\n",
        "            # Specify all columns as numeric for Gower distance\n",
        "            gower_distances = gower.gower_matrix(vectors_df_filtered)\n",
        "\n",
        "            # Calculate silhouette scores\n",
        "            silhouette_vals = silhouette_samples(\n",
        "                gower_distances,\n",
        "                week_cluster_labels[mask],\n",
        "                metric=\"precomputed\"\n",
        "            )\n",
        "\n",
        "            # Calculate scores per cluster\n",
        "            cluster_silhouette_scores = {}\n",
        "            for cluster in non_noise_labels:\n",
        "                cluster_mask = week_cluster_labels[mask] == cluster\n",
        "                cluster_silhouette_vals = silhouette_vals[cluster_mask]\n",
        "                mean_silhouette_score = cluster_silhouette_vals.mean()\n",
        "                cluster_silhouette_scores[cluster] = mean_silhouette_score\n",
        "\n",
        "                # Get the texts for this cluster\n",
        "                cluster_texts = week_data[week_data['cluster'] == cluster]['text'].values\n",
        "\n",
        "                print(f\"\\nCluster {cluster}:\")\n",
        "                print(f\"Silhouette Score: {mean_silhouette_score:.3f}\")\n",
        "                print(f\"Number of texts: {len(cluster_texts)}\")\n",
        "                print(f\"Score range: {cluster_silhouette_vals.min():.3f} to {cluster_silhouette_vals.max():.3f}\")\n",
        "\n",
        "                # Calculate mean Gower distance within cluster\n",
        "                cluster_indices = np.where(week_cluster_labels[mask] == cluster)[0]\n",
        "                if len(cluster_indices) > 1:\n",
        "                    cluster_distances = gower_distances[cluster_indices][:, cluster_indices]\n",
        "                    mean_dist = np.mean(cluster_distances[np.triu_indices_from(cluster_distances, k=1)])\n",
        "                    print(f\"Mean internal Gower distance: {mean_dist:.3f}\")\n",
        "\n",
        "                print(\"Sample texts:\")\n",
        "                print(cluster_texts[:3])\n",
        "\n",
        "            # Store results for clusters\n",
        "            cc_per_cluster = {}\n",
        "            for cluster, score in cluster_silhouette_scores.items():\n",
        "                cluster_texts = week_data[week_data['cluster'] == cluster]['text'].values\n",
        "\n",
        "                cc_per_cluster[cluster] = {\n",
        "                    'texts': cluster_texts,\n",
        "                    'silhouette_score': score,\n",
        "                    'size': len(cluster_texts),\n",
        "                    'sample_texts': cluster_texts[:5]\n",
        "                }\n",
        "\n",
        "            # Calculate overall silhouette score for the week\n",
        "            overall_score = silhouette_score(\n",
        "                gower_distances,\n",
        "                week_cluster_labels[mask],\n",
        "                metric=\"precomputed\"\n",
        "            )\n",
        "            print(f\"\\nOverall week silhouette score: {overall_score:.3f}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Insufficient unique clusters or samples for week {week_start}\")\n",
        "            print(f\"Number of non-noise points: {sum(mask)}\")\n",
        "            print(f\"Number of unique non-noise clusters: {len(non_noise_labels)}\")\n",
        "            cc_per_cluster = {}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing week {week_start}:\")\n",
        "        print(f\"Error details: {str(e)}\")\n",
        "        print(\"Data shape:\", vectors.shape if 'vectors' in locals() else \"No vectors\")\n",
        "        cc_per_cluster = {}\n",
        "\n",
        "    weekly_cc_per_cluster[week_start] = cc_per_cluster\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nOverall Results Summary:\")\n",
        "for week, clusters in weekly_cc_per_cluster.items():\n",
        "    if clusters:\n",
        "        print(f\"\\nWeek {week}\")\n",
        "        print(f\"Number of clusters: {len(clusters)}\")\n",
        "\n",
        "        scores = [info['silhouette_score'] for info in clusters.values()]\n",
        "        if scores:\n",
        "            print(f\"Average silhouette score: {np.mean(scores):.3f}\")\n",
        "            print(f\"Min silhouette score: {min(scores):.3f}\")\n",
        "            print(f\"Max silhouette score: {max(scores):.3f}\")\n",
        "\n",
        "            # Calculate cluster sizes\n",
        "            sizes = [info['size'] for info in clusters.values()]\n",
        "            print(f\"Average cluster size: {np.mean(sizes):.1f}\")\n",
        "            print(f\"Min cluster size: {min(sizes)}\")\n",
        "            print(f\"Max cluster size: {max(sizes)}\")\n",
        "\n",
        "        for cluster_id, cluster_info in clusters.items():\n",
        "            print(f\"\\n  Cluster {cluster_id}:\")\n",
        "            print(f\"  Size: {cluster_info['size']}\")\n",
        "            print(f\"  Silhouette Score: {cluster_info['silhouette_score']:.3f}\")\n",
        "            if len(cluster_info['sample_texts']) > 0:\n",
        "                print(\"  Sample text:\", cluster_info['sample_texts'][0])"
      ],
      "metadata": {
        "id": "PyhHmnZkVGSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOvcpAQhfgCdTVdqIQPyGsf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}